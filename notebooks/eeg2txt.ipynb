{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from peft import AdaptionPromptConfig, PeftModel, get_peft_model\n",
    "from torch import nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from toto.inference.embedding import embed\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "# torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba709c",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_ROOT = \"../adalog-sessions/phil\"\n",
    "TAGS = [\"eeg\", \"writing\", \"intuitive\"]\n",
    "# SESSION_ROOT = \"../adalog-sessions/Antoine\"\n",
    "# TAGS = [\"AutomaticWriting\"]\n",
    "\n",
    "SFREQ = 256  # Hz\n",
    "PRE_ONSET_DURATION = 0.25  # seconds\n",
    "POST_ONSET_DURATION = 0.75  # seconds\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B\"\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 7\n",
    "ADAPTER_LEN = 16\n",
    "ADAPTER_LAYERS = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_TRAIN_CONTEXT_WORDS = 20\n",
    "\n",
    "uname = Path(SESSION_ROOT).name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d52c6",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "### Load EEG + text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(root: str, modalities: List[str], tags: List[str] = [], file_glob: str = \"*\") -> Dict[str, List[str]]:\n",
    "    if not isinstance(modalities, list):\n",
    "        modalities = [modalities]\n",
    "    if not isinstance(tags, list):\n",
    "        tags = [tags]\n",
    "\n",
    "    root = Path(SESSION_ROOT)\n",
    "    paths = {}\n",
    "    for tag_path in root.glob(f\"**/tags.csv\"):\n",
    "        # check if all tags are present\n",
    "        ts = [t for t in pd.read_csv(tag_path)[\"tags\"].tolist() if isinstance(t, str)]\n",
    "        ts = sum(map(partial(str.split, sep=\", \"), ts), [])\n",
    "        if len(set(tags).intersection(set(ts))) < len(tags):\n",
    "            # make sure all tags are present\n",
    "            continue\n",
    "\n",
    "        # check if all modalities are present\n",
    "        ms = [p.name for p in tag_path.parent.glob(\"*\") if p.is_dir()]\n",
    "        if len(set(modalities).intersection(set(ms))) < len(modalities):\n",
    "            # make sure all modalities are present\n",
    "            continue\n",
    "\n",
    "        # get paths per modality\n",
    "        for modality in modalities:\n",
    "            if modality not in paths:\n",
    "                paths[modality] = []\n",
    "            paths[modality].extend(list(tag_path.parent.glob(f\"{modality}/{file_glob}\")))\n",
    "    return {k: sorted([str(p) for p in v]) for k, v in paths.items()}\n",
    "\n",
    "\n",
    "paths = get_paths(SESSION_ROOT, [\"Text\", \"Eeg\"], TAGS, file_glob=\"*.csv\")\n",
    "txt_paths = paths[\"Text\"]\n",
    "eeg_paths = paths[\"Eeg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d14ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp2index(ts: pd.Timestamp, df: pd.DataFrame, max_diff: float = 0.2) -> int:\n",
    "    \"\"\"Find the index of the closest timestamp in a DataFrame, within max_diff seconds.\"\"\"\n",
    "    diffs = (df[\"timestamp\"] - ts).abs()\n",
    "    min_diff = diffs.min().total_seconds()\n",
    "    if min_diff > max_diff:\n",
    "        raise ValueError(f\"No timestamp within {max_diff} seconds (closest: {min_diff:.3f}s)\")\n",
    "    return diffs.argsort()[0]\n",
    "\n",
    "\n",
    "def process_row(tdf_row: pd.Series, edf: pd.DataFrame) -> Optional[Tuple[str, int, pd.DataFrame]]:\n",
    "    try:\n",
    "        \"\"\"Process a single row of text data and extract the corresponding EEG segment.\"\"\"\n",
    "        # get a word and its timestamp\n",
    "        ts, word = tdf_row[[\"timestamp\", \"content\"]]\n",
    "\n",
    "        # find the corresponding sample indices in the EEG data\n",
    "        try:\n",
    "            onset_idx = timestamp2index(ts, edf)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: {e} for word '{word}' at {ts}. Skipping this segment.\")\n",
    "            return None\n",
    "        pre_idx = onset_idx - int(PRE_ONSET_DURATION * SFREQ)\n",
    "        post_idx = onset_idx + int(POST_ONSET_DURATION * SFREQ)\n",
    "\n",
    "        if pre_idx < 0 or post_idx >= len(edf):\n",
    "            print(\n",
    "                f\"Warning: Out of bounds for word '{word}' at {ts}. \"\n",
    "                f\"Pre-index: {pre_idx}, Post-index: {post_idx}, Length: {len(edf)}. Skipping this segment.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # extract the EEG data around the word onset\n",
    "        eeg_segment = edf.iloc[pre_idx:post_idx].copy()\n",
    "        eeg_segment.drop(columns=[\"timestamp\"], inplace=True)\n",
    "        eeg_segment = eeg_segment.values.astype(np.float32)\n",
    "\n",
    "        expected_length = int((PRE_ONSET_DURATION + POST_ONSET_DURATION) * SFREQ)\n",
    "        assert (\n",
    "            len(eeg_segment) == expected_length\n",
    "        ), f\"Expected EEG segment length {expected_length}, but got {len(eeg_segment)} for word '{word}' at {ts}.\"\n",
    "        return word, onset_idx - pre_idx, eeg_segment\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing word '{tdf_row['content']}' at {tdf_row['timestamp']}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "data = []\n",
    "print(f\"Processing {len(txt_paths)} text + EEG files...\")\n",
    "for tp, ep in tqdm(zip(txt_paths, eeg_paths), total=len(txt_paths)):\n",
    "    # load text and EEG data\n",
    "    tdf = pd.read_csv(tp, parse_dates=[\"timestamp\"])\n",
    "    edf = pd.read_csv(ep, parse_dates=[\"timestamp\"])\n",
    "\n",
    "    chunks = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row)(row, edf) for _, row in tqdm(tdf.iterrows(), total=len(tdf), leave=False)\n",
    "    )\n",
    "    n = len(chunks)\n",
    "    chunks = list(filter(None, chunks))\n",
    "    print(f\"Processed {n} words, discarded {n - len(chunks)} due to out-of-bounds or errors.\")\n",
    "    data.extend(chunks)\n",
    "\n",
    "print(f\"\\nTotal number of valid word+EEG pairs: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bcd108",
   "metadata": {},
   "source": [
    "### Embed EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_segments_path = f\"eeg_segments-{uname}-n={len(data)}.pkl\"\n",
    "if path.exists(eeg_segments_path):\n",
    "    print(f\"Loading existing EEG segments from '{eeg_segments_path}'...\")\n",
    "    with open(eeg_segments_path, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "else:\n",
    "    df = pd.DataFrame(data, columns=[\"word\", \"onset_idx\", \"eeg\"])\n",
    "    df[\"embedding\"] = None\n",
    "    for i in trange(len(df), desc=\"Embedding EEG segments\"):\n",
    "        word, onset_idx, eeg_segment, _ = df.iloc[i]\n",
    "        embedded = embed(eeg_segment.T)\n",
    "        df.at[i, \"embedding\"] = embedded.mean(dim=(0, 1, 2)).numpy()\n",
    "\n",
    "    print(f\"Saving EEG segments to '{eeg_segments_path}'...\")\n",
    "    with open(eeg_segments_path, \"wb\") as f:\n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233154e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGAdapter(nn.Module):\n",
    "    \"\"\"Wraps a causal-LM with an AdaptionPrompt PEFT adapter and a learnable\n",
    "    linear projection that turns a 768-d EEG embedding into *adapter_len* extra\n",
    "    prefix tokens.\n",
    "\n",
    "    The class handles forward logic **and** self-contained save / load routines\n",
    "    so that the adapter + EEG projection can be checkpointed and restored with\n",
    "    a single folder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, adapter_cfg: AdaptionPromptConfig, eeg_embed_dim: int = 768):\n",
    "        super().__init__()\n",
    "        self.base_model = get_peft_model(base_model, adapter_cfg)\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.adapter_len = adapter_cfg.adapter_len\n",
    "\n",
    "        # learnable projection from EEG embedding to adapter prefix tokens\n",
    "        self.eeg_proj = nn.Linear(eeg_embed_dim, self.hidden_size * self.adapter_len)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, eeg_embed=None, past_key_values=None, use_cache=False):\n",
    "        \"\"\"If *past_key_values* is *None* (first time step) we inject the EEG\n",
    "        prefix. On subsequent calls we just pass the single token to the base\n",
    "        model together with *past_key_values*.\n",
    "        \"\"\"\n",
    "        if past_key_values is None:  # TODO: we probably always want to pass the eeg_embed, not only on the first step\n",
    "            # project EEG embedding to prefix dimension\n",
    "            assert eeg_embed is not None, \"Missing EEG embedding for the first step\"\n",
    "            bs = input_ids.size(0)\n",
    "            prefix = self.eeg_proj(eeg_embed).view(bs, self.adapter_len, self.hidden_size)\n",
    "\n",
    "            # update base embeddings with EEG prefix\n",
    "            tok_emb = self.base_model.base_model.get_input_embeddings()(input_ids)\n",
    "            inputs_embeds = torch.cat([prefix, tok_emb], dim=1)\n",
    "\n",
    "            # update attention mask to account for EEG prefix\n",
    "            if attention_mask is None:\n",
    "                attention_mask = torch.ones_like(input_ids)\n",
    "            pref_mask = torch.ones(bs, self.adapter_len, dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([pref_mask, attention_mask], dim=1)\n",
    "\n",
    "            # base model forward pass with EEG prefix\n",
    "            return self.base_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, use_cache=use_cache)\n",
    "        else:\n",
    "            # TODO: we probably always want to pass the eeg_embed, not only on the first step\n",
    "            return self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # adapter weights + config\n",
    "        self.base_model.save_pretrained(save_directory)\n",
    "        # EEG projection\n",
    "        torch.save(self.eeg_proj.state_dict(), os.path.join(save_directory, \"eeg_proj.bin\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, base_model, load_directory: str, eeg_embed_dim: int = 768):\n",
    "        # attach PEFT adapter from load_directory to the base model\n",
    "        peft_model = PeftModel.from_pretrained(base_model, load_directory)\n",
    "        peft_cfg = peft_model.peft_config[\"default\"]\n",
    "\n",
    "        # instantiate and configure EEGAdapter\n",
    "        instance = cls.__new__(cls)\n",
    "        nn.Module.__init__(instance)\n",
    "        instance.base_model = peft_model\n",
    "        instance.hidden_size = base_model.config.hidden_size\n",
    "        instance.adapter_len = peft_cfg.adapter_len\n",
    "        instance.eeg_proj = nn.Linear(eeg_embed_dim, instance.hidden_size * instance.adapter_len)\n",
    "\n",
    "        # apply state dict to the EEG projection layer\n",
    "        eeg_state = torch.load(os.path.join(load_directory, \"eeg_proj.bin\"), map_location=\"cpu\")\n",
    "        instance.eeg_proj.load_state_dict(eeg_state)\n",
    "        return instance\n",
    "\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGLMDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Returns only input_ids + per-token EEG; labels will be built in the collator.\n",
    "    `df` must expose columns 'word' and 'embedding' (768-d list).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, model_id, ctx_words=30):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        self.words = df[\"word\"].tolist()\n",
    "        self.eegs = df[\"embedding\"].tolist()\n",
    "        self.ctx = ctx_words\n",
    "        self.items = []\n",
    "\n",
    "        for st in trange(0, len(self.words) - ctx_words, desc=\"Building dataset\"):\n",
    "            w_chunk = [\" \".join(self.words[st : st + ctx_words])]\n",
    "            e_chunk = self.eegs[st : st + ctx_words]\n",
    "\n",
    "            enc = self.tokenizer(w_chunk, is_split_into_words=True, add_special_tokens=False, return_attention_mask=False)\n",
    "            w_ids = enc.word_ids()\n",
    "            eeg_seq = np.array([e_chunk[w] for w in w_ids])  # repeat per sub-token\n",
    "\n",
    "            self.items.append(\n",
    "                {\n",
    "                    \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
    "                    \"eeg_embed\": torch.from_numpy(eeg_seq.astype(np.float32)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.items[i]\n",
    "\n",
    "\n",
    "class EEGCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_text = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        txt = self.pad_text([{\"input_ids\": b[\"input_ids\"]} for b in batch])\n",
    "        L = txt[\"input_ids\"].shape[1]\n",
    "        D = batch[0][\"eeg_embed\"].shape[-1]\n",
    "\n",
    "        txt[\"labels\"] = txt[\"input_ids\"].clone()  # ← build labels\n",
    "\n",
    "        eeg = torch.zeros(len(batch), L, D)\n",
    "        for i, b_ in enumerate(batch):\n",
    "            n = b_[\"eeg_embed\"].shape[0]\n",
    "            eeg[i, -n:, :] = b_[\"eeg_embed\"]  # left-pad\n",
    "        txt[\"eeg_embed\"] = eeg\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ds = EEGLMDataset(df, model_id=MODEL_ID, ctx_words=NUM_TRAIN_CONTEXT_WORDS)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=EEGCollator(ds.tokenizer))\n",
    "\n",
    "# base LLM\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=DEVICE, quantization_config=bnb_cfg)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_toke = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# EEG adapter\n",
    "adapter_cfg = AdaptionPromptConfig(task_type=\"CAUSAL_LM\", adapter_len=ADAPTER_LEN, adapter_layers=ADAPTER_LAYERS)\n",
    "eeg_adapter = EEGAdapter(base_model, adapter_cfg).to(DEVICE)\n",
    "# eeg_adapter.compile() # TODO: currently we hit the recompile limit due to the growing past_key_values, maybe increasing the recompile limit is enough\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(eeg_adapter.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_adapter.train()\n",
    "\n",
    "try:\n",
    "    for epoch in range(1):\n",
    "        epoch_pbar = tqdm(dl, desc=f\"epoch {epoch}\")\n",
    "        for i, batch in enumerate(epoch_pbar):\n",
    "            eeg_seq = batch.pop(\"eeg_embed\")[:, :-1].to(DEVICE)\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"][:, 1:].to(DEVICE)\n",
    "\n",
    "            past, loss = None, 0.0\n",
    "            eeg_adapter.zero_grad(set_to_none=True)\n",
    "\n",
    "            for t in trange(input_ids.size(1) - 1, leave=False, desc=\"steps\"):\n",
    "                with autocast(device_type=DEVICE):\n",
    "                    out = eeg_adapter(\n",
    "                        input_ids=input_ids[:, t : t + 1],\n",
    "                        eeg_embed=(\n",
    "                            eeg_seq[:, t, :] if past is None else None\n",
    "                        ),  # TODO: verify whether we only want to pass the eeg_vec on the first step\n",
    "                        past_key_values=past,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    past = out.past_key_values\n",
    "\n",
    "                    logits_step = out.logits[:, -1, :]\n",
    "                    loss_step = nn.functional.cross_entropy(logits_step, labels[:, t])\n",
    "                    loss += loss_step / (input_ids.size(1) - 1)\n",
    "\n",
    "            epoch_pbar.set_postfix({\"loss\": loss.item()})\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted.\")\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7943a6",
   "metadata": {},
   "source": [
    "### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = f\"eeg_llama_adapter-{uname}\"\n",
    "eeg_adapter.save_pretrained(ckpt_dir)\n",
    "tokenizer.save_pretrained(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078abdff",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d3ae5",
   "metadata": {},
   "source": [
    "### Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = f\"eeg_llama_adapter-{uname}\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=DEVICE, quantization_config=bnb_cfg)\n",
    "eeg_adapter = EEGAdapter.from_pretrained(base_model, ckpt_dir).eval().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = tokenizer.eos_token_id\n",
    "space_id = tokenizer(\" \", add_special_tokens=False)[\"input_ids\"][0]\n",
    "eeg_adapter.eval()\n",
    "\n",
    "\n",
    "def nucleus_sampling(logits, top_p=0.95, temperature=1.0):\n",
    "    \"\"\"Apply nucleus (top-p) sampling to logits.\"\"\"\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "    # Sort probabilities in descending order\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    # Create mask for tokens to keep (cumulative prob <= top_p)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Always keep at least the top token\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "    # Zero out probabilities for tokens to remove\n",
    "    sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "    # Renormalize\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "    # Sample from the filtered distribution\n",
    "    sampled_sorted_indices = torch.multinomial(sorted_probs, 1)\n",
    "    # Map back to original indices\n",
    "    next_token = sorted_indices.gather(-1, sampled_sorted_indices)\n",
    "    return next_token.squeeze(-1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_word_from_eeg(\n",
    "    eeg_vec: torch.Tensor,\n",
    "    max_tokens_per_word: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    context_ids: list = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate one word conditioned on a 768-d EEG latent vector.\n",
    "\n",
    "    Args:\n",
    "        eeg_vec: EEG embedding tensor of shape (768,) or (1, 768)\n",
    "        max_tokens_per_word: Maximum tokens to generate before stopping\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling threshold\n",
    "        context_size: Maximum context length in tokens\n",
    "        context_ids: Previous token IDs to continue from (for multi-word generation)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (word_string, updated_context_ids)\n",
    "    \"\"\"\n",
    "    # ensure proper tensor shape and device\n",
    "    if eeg_vec.ndim == 1:\n",
    "        eeg_vec = eeg_vec.unsqueeze(0)  # (1, 768)\n",
    "    eeg_vec = eeg_vec.to(DEVICE)\n",
    "\n",
    "    # initialize context\n",
    "    if context_ids is None:\n",
    "        context_ids = [eos_id]\n",
    "    past_key_values = None\n",
    "\n",
    "    for _ in range(max_tokens_per_word):\n",
    "        # forward pass\n",
    "        with autocast(device_type=DEVICE):\n",
    "            outputs = eeg_adapter(\n",
    "                input_ids=torch.tensor([[context_ids[-1]]], device=DEVICE).long(),\n",
    "                eeg_embed=(\n",
    "                    eeg_vec if past_key_values is None else None\n",
    "                ),  # TODO: verify whether we only want to pass the eeg_vec on the first step\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,  # TODO: potentially limit the context size\n",
    "            )\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # sample next token\n",
    "        logits = outputs.logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "        next_token_id = nucleus_sampling(logits, top_p=top_p, temperature=temperature).item()\n",
    "        context_ids.append(next_token_id)\n",
    "\n",
    "        # stop if we hit a space (end of word) or EOS\n",
    "        if next_token_id == space_id or next_token_id == eos_id:\n",
    "            break\n",
    "    return context_ids\n",
    "\n",
    "\n",
    "def generate_text_from_eeg_sequence(embeddings, preprompt=None, max_words=None, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    Generate text from a sequence of EEG embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: List or array of EEG embedding vectors\n",
    "        max_words: Maximum number of words to generate (None for all embeddings)\n",
    "        **generation_kwargs: Arguments passed to generate_word_from_eeg\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    if max_words is not None:\n",
    "        embeddings = embeddings[:max_words]\n",
    "\n",
    "    if preprompt is None:\n",
    "        context_ids = None\n",
    "    else:\n",
    "        context_ids = tokenizer(preprompt, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    for z in embeddings:\n",
    "        if not isinstance(z, torch.Tensor):\n",
    "            z = torch.from_numpy(z)\n",
    "        context_ids = generate_word_from_eeg(z, context_ids=context_ids, **generation_kwargs)\n",
    "\n",
    "        # update output stream\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(tokenizer.decode(context_ids, skip_special_tokens=True)))\n",
    "    return tokenizer.decode(context_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Generate from first 10 embeddings with progress display\n",
    "generated_text = generate_text_from_eeg_sequence(\n",
    "    df[\"embedding\"].tolist()[:10],\n",
    "    preprompt=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
